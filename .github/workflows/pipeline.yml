name: Data Platform Pipeline
on:
  # repository_dispatch:
  #   types: [run-ingestion]
  push:
    branches: [main]       # Syncs UI changes to HF immediately
    paths-ignore:
      - 'data/**'
  repository_dispatch:
    types: [run-ingestion] # Triggered by the Streamlit button for data
  # schedule:
  #   - cron: '0 * * * *'    # Every hour on the hour

concurrency:
  group: data-platform
  cancel-in-progress: false # This ensures the first user's run finishes safely


jobs:
  ingest-and-transform:
    if: github.event_name == 'repository_dispatch'
    runs-on: ubuntu-latest
    permissions: # Required to push data back to the repo
      contents: write
    steps:
      - uses: actions/checkout@v4
        with:
          fetch-depth: 0 # Needed for committing back

      - name: Create data directories
        run: mkdir -p data/bronze

      - name: Setup Rust
        uses: dtolnay/rust-toolchain@stable

      - name: Run Rust Scraper
        run: |
          mkdir -p data/bronze
          cd scraper
          cargo run --release
        env:
          DB_CLIENT_ID: ${{ secrets.DB_CLIENT_ID }}
          DB_API_KEY: ${{ secrets.DB_API_KEY }}

      - name: Setup Python & dbt
        uses: actions/setup-python@v4
        with:
          python-version: '3.10'

      - name: Install dbt
        run: pip install dbt-duckdb

      - name: Run dbt (Silver & Gold)
        run: |
          dbt deps
          dbt run --target prod --profiles-dir .

      # COMMIT THE DATA 
      - name: Commit and Push updated data
        run: |
          git config --global user.name "github-actions[bot]"
          git config --global user.email "github-actions[bot]@users.noreply.github.com"
          git add data/
          # Only commit if there are actually changes in the data folder
          git diff --quiet && git diff --staged --quiet || (git commit -m "Update data: $(date)" && git push)

  sync-to-hub:
    if: |
      always() && 
      (github.event_name == 'push' || needs.ingest-and-transform.result == 'success')
    needs: ingest-and-transform
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4
        with:
          fetch-depth: 0

      - name: Pull latest data from GitHub
        run: git pull origin main

      - name: Install HF Hub CLI
        run: pip install huggingface_hub

      - name: Upload Database and Data and Streamlit to Space
        env:
          HF_TOKEN: ${{ secrets.HF_ACCESS_TOKEN }}
        run: |
          # This uploads the specific files directly via API, bypassing the Git binary block
          python -c "
          from huggingface_hub import HfApi
          api = HfApi()
          repo_id = 'nde97/punktlich-train-analytics-nde97'

          # Upload the updated app.py (The Code)
          api.upload_file(
              path_or_fileobj='app.py',
              path_in_repo='app.py',
              repo_id=repo_id,
              repo_type='space',
              token='${{ env.HF_TOKEN }}'
          )

          # Upload the database
          api.upload_file(
              path_or_fileobj='data/dbt.duckdb',
              path_in_repo='data/dbt.duckdb',
              repo_id=repo_id,
              repo_type='space',
              token='${{ env.HF_TOKEN }}'
          )
          # Upload the Parquet folder
          api.upload_folder(
              folder_path='data/bronze',
              path_in_repo='data/bronze',
              repo_id=repo_id,
              repo_type='space',
              token='${{ env.HF_TOKEN }}'
          )
          "
